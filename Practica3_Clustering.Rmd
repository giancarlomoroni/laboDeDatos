---
title: "Practica3_Clustering"
output: html_document
date: "2023-02-22"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r}
library(ggplot2)
library(gridExtra)
set.seed(12345)
x <- runif(1000,0,1)
hist(x)
y <- rnorm(1000,0,1)
hist(y)
```

```{r}
kMeans2 <- function(a,b,k.clus){
  
  #Limpio los NAs
  data <- data.frame("X" = a, "Y" = b, "Cluster" = rep(NA,length(a)))
  data <- data[!is.na(data$X) & !is.na(data$Y),]
  ggplot(data,aes(X,Y))+geom_jitter()
  
  #Creo empty dataframe
  columns = c("X","Y") 
  centroides = data.frame(matrix(nrow = 0, ncol = length(columns))) 
  colnames(centroides) = columns
  
  random <- sample(1:length(a),k.clus,replace = F)
  
  for (i in 1:k.clus){
    centroides <- rbind(centroides,data[random[i],1:2])
  }
  
  centroidesAnteriores <- data.frame("X" = rep(0,k.clus),"Y"=rep(0,k.clus))
  
  calculo <- sqrt(apply((centroides-centroidesAnteriores)**2,1,sum))
  
  ggplot(centroides,aes(X,Y))+geom_point(shape=4,color="blue",size=4)
  matriz <- matrix(rep(NA,dim(data)[1] * k.clus), nrow=dim(data)[1],ncol = k.clus)
  
  #Bucle
  iter <- 0
  distanciaFinal <- c()
  while(max(calculo) > 0.05){
    for (i in 1:dim(data)[1]){
      for(j in 1:k.clus){
        matriz[i,j] <- distancia(data[i,1:2],centroides[j,])
      }
    }
    cat <- apply(X = matriz,MARGIN= 1,FUN = which.min)
    distanciaFinal[iter+1] <- sum(apply(X = matriz,MARGIN= 1,FUN = min))
    
    data[,"Cluster"] <- factor(cat)
    
    centroidesAnteriores <- centroides
    
    #Nuevos centros
    for (centro in 1:k.clus) {
      centroides[centro,] <- c(mean(data[data$Cluster==centro,"X"]),mean(data[data$Cluster==centro,"Y"]))
    }
    calculo <- sqrt(apply((centroides-centroidesAnteriores)**2,1,sum))
    
    iter <- iter + 1
  }
  
  error <- data.frame("X" = 1:length(distanciaFinal),"Y" = distanciaFinal)
  g1 <-ggplot(error,aes(X,Y))+geom_jitter()
  
  g2 <- ggplot(data,aes(X,Y,color=Cluster))+geom_jitter() + 
    geom_point(data=centroides,aes(X,Y),shape=4,size=4,color="blue")+xlab("X")+ylab("Y")+
    ggtitle("Cluster")
  
  
  return(list(data$Cluster,centroides,distanciaFinal[length(distanciaFinal)],g2))
}

```


```{r}
xA <- runif(50,2,4)
yA <- runif(50,1,2)

xB <- rnorm(50,0,1)
yB <- rnorm(50,0,1)

xC <- runif(50,1,2)
yC <- runif(50,-1,2)

color <- c(rep(1,50),rep(2,50),rep(3,50))
color <- factor(color)
data <- data.frame(x = c(xA,xB,xC),y=c(yA,yB,yC), cat = color)
g1 <- ggplot(data,aes(x,y,color = cat))+geom_point() +ggtitle("Categoria Correcta")

```

# Ahora vamos a hacer el clustering

```{r}
as.matrix(data)
cluster <- kmeans(as.matrix(data),3)[[1]]
cluster <- factor(cluster)
g2 <- ggplot(data,aes(x,y,color = cluster))+geom_point()+ggtitle("Categoria Kmeans")
grid.arrange(g1,g2)
```

# Punto B

```{r}
tA <- runif(100,0,2*pi)
rA <- runif(100,0,1)

xA2 <- rA*cos(tA)
yA2 <- rA*sin(tA)

tB <- runif(100,0,2*pi)
rB <- runif(100,2,2.5)

xB2 <- rB*cos(tB)
yB2 <- rB*sin(tB)

col2 <- factor(c(rep(1,100),rep(2,100)))

data2 <- data.frame(x = c(xA2,xB2),y=c(yA2,yB2), cluster = col2)
ggplot(data2,aes(x,y,color = cluster)) + geom_point()

```
# Hago el cluster por kmeans

```{r}
segmentacionMala <- factor(kmeans(data2,center = 2)[[1]])

ggplot(data2,aes(x,y,color = segmentacionMala)) + geom_point()

```

Se concluye que para ciertos casos kmedias no logra segmentar correctamente ya que se utiliza la distancia euclidea y no tiene cierto sentido en algunos casos

# Ejercicio 2

```{r}
producto <- read.csv("producto.txt",sep = " ")
producto$Resultado <- factor(producto$Resultado)
ggplot(producto,aes(Marketing,Precio))+geom_point()
ggplot(producto,aes(Marketing,Precio,color=Resultado))+geom_point()+ggtitle("Clasificacion Real")
cat <- factor(kmeans(as.matrix(producto[,2:3]),center =2)[[1]])
ggplot(producto,aes(Marketing,Precio,color=cat))+geom_point()+ggtitle("Clasificacion por Kmeans")
table(producto$Resultado,cat)
```

Como vemos que hay puntos que no estan siendo clasificados correctamente, una primera suposicion es que los puntos de la derecha estan muy lejanos en terminos de x del cluster de la derecha, y esto se debe a la diferencia en magnitud de las unidades.

Por eso hacemos feature scaling, para eso utlizaremos estandarizacion

```{r}
newProducto <- producto
newProducto$Precio <- (newProducto$Precio - mean(newProducto$Precio))/sd(newProducto$Precio)
newProducto$Marketing <- (newProducto$Marketing - mean(newProducto$Marketing))/sd(newProducto$Marketing)

ggplot(newProducto,aes(Marketing,Precio,color=Resultado))+geom_point()+ggtitle("Categoria Reales con Estandarizacion")
cat2 <- factor(kmeans(as.matrix(newProducto[,2:3]),center =2)[[1]])
ggplot(newProducto,aes(Marketing,Precio,color=cat2))+geom_point()+ggtitle("Categoria kMeans con Estandarizacion")
table(newProducto$Resultado,cat2)

```


```{r}
newProducto2 <- producto
newProducto2$Precio <- (newProducto2$Precio - min(newProducto2$Precio))/(max(newProducto2$Precio) - min(newProducto2$Precio))
newProducto2$Marketing <- (newProducto2$Marketing - min(newProducto2$Marketing))/(max(newProducto2$Marketing) - min(newProducto2$Marketing))

ggplot(newProducto2,aes(Marketing,Precio,color=Resultado))+geom_point()+ggtitle("Categoria Reales con Normalizacion")
cat3 <- factor(kmeans(as.matrix(newProducto2[,2:3]),center =2)[[1]])
ggplot(newProducto2,aes(Marketing,Precio,color=cat3))+geom_point()+ggtitle("Categoria kMeans con Normalizacion")
table(newProducto2$Resultado,cat3)
```

Vemos el Cluster tiene solo 4 errores comparado al real, por lo que podemos decir que al normalizar o estandarizar los datos, ganamos una mayor fiabilidad al hacer clustering.

# Ejercicio 3

```{r}
arrestos <- USArrests
#Primero lo que hago es estandarizar el dataset para obtener un mejor desempeno en el kmedias.
for (i in 1:dim(arrestos)[2]) {
    arrestos[,i] <- (arrestos[,i] - mean(arrestos[,i]))/sd(arrestos[,i]) 
}

clus3 <- factor(kmeans(arrestos,center = 3)[[1]])
clus4 <- factor(kmeans(arrestos,center = 4)[[1]])
clus5 <- factor(kmeans(arrestos,center = 5)[[1]])

#ELBOW METHOD

suma <- c()
for(i in 1:10){
  suma[i] <- kmeans(arrestos,center = i)[[5]] #el 5 te da la distancia de los puntos en cada cluster
}

ggplot(as.data.frame(suma),aes(1:length(suma),suma))+geom_line(color="red")+geom_point()+theme_light()+ggtitle("Elbow Method")

#AVERAGE SILHOUETTE METHOD
library(factoextra)
fviz_nbclust(arrestos,kmeans,method="silhouette")

#GAP STATISTIC
fviz_nbclust(df, kmeans, nstart = 25,  method = "gap_stat", nboot = 50)+
  labs(subtitle = "Gap statistic method")

```


```{r}
#Analsis para un k = 3
library(plotly)
plot_ly(x = arrestos$Murder,y=arrestos$Assault,z=arrestos$Rape,color = clus3)

#Analisis para un k = 4
plot_ly(x = arrestos$Murder,y=arrestos$Assault,z=arrestos$Rape,color = clus4)

#Analisis para un k = 5
plot_ly(x = arrestos$Murder,y=arrestos$Assault,z=arrestos$Rape,color = clus5)

```

Veo que el cluster que tiene en general mejor rendimiento es para k = 4, por lo tanto voy a hacer un analisis de este para cada cluster.

```{r}
#Voy a analizar el promedio de todas las variables en funcion de su cluster y listarlo en un data frame, ya que todas las variables son numericas y tienen una representacion ese numero y no nos sirviria ver la mediana.

columns = c("Cluster","Murder","Assault","UrbanPop","Rape") 
resumen = data.frame(matrix(nrow = 0, ncol = length(columns))) 
for (i in 1:4) {
  vec <- c()
  vec[1] <- i
  for (j in 1:4) {
    
  }
}


```






